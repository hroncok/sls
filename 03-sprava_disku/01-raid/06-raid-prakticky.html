<h2>Softwarový RAID prakticky</h2>

<h3>Průzkum existujícího pole</h3>

<p>Informace o všech diskových polích spravovaných Linuxem naleznete v souboru <code>/proc/mdstat</code>. Výpis tohoto souboru může vypadat třeba takto:</p>

<pre>debian:~# cat /proc/mdstat
Personalities : [raid1] 
md2 : active raid1 sdb2[0] sda2[1]
      975763866 blocks super 1.2 [2/2] [UU]
      bitmap: 5/466 pages [20KB], 1024KB chunk
md0 : active raid1 sdb1[0] sda1[1]
      995904 blocks [2/2] [UU]
unused devices: &ls;none&gt;</pre>

<p>V tomto výpisu jsou vidět dvě pole, RAID 1 pole <code>md0</code> o přibližné velikosti 1GB složené ze zařízení <code>sdb1</code> a <code>sda1</code> a RAID 1 pole <code>md2</code> o přibližné velikosti 1TB složené ze zařízení <code>sdb2</code> a <code>sda2</code>. Obě dvě pole jsou aktivní se dvěma ze dvou zařízení (viz <code>[2/2]</code>).</p>

<p>Zkuste se podívat na jiný výpis:</p>

<pre>debian:~# cat /proc/mdstat
Personalities : [raid1] 
md0 : active raid1 hda1[0] hdd1[3](F) hdb1[1]
      1172608 blocks [3/2] [UU_]
      
unused devices: &ls;none&gt;</pre>

<p>Zde je patrný problém se zařízením <code>hdd1</code>, které bylo označeno jako vadné. Proto je u něj písmeno <strong>F</strong> (f jako faulty). Pole je aktivní, ale pouze se dvěma ze tří zařízení (viz <code>[3/2]</code>). Povšimněte si ve výpisu posloupnosti označující stav jednotlivých zařízení seřazených podle jejich pořadí v poli - <code>[UU_]</code>. Zde značí písmeno <code>U</code> aktivní zařízení v poli a podtržítko značí chybějící aktivní zařízení.</p>

<p>V momentě, kdy chybné zařízení vyřadíte z pole a zařadíte jeho náhradu, začne rekonstrukce pole:</p>

<pre>debian:~# cat /proc/mdstat
Personalities : [raid1] 
md0 : active raid1 hdd1[3] hda1[0] hdb1[1]
      1172608 blocks [3/2] [UU_]
      [=====&gt;...............]  recovery = 28.0% (328768/1172608) finish=0.7min speed=18264K/sec

unused devices: &ls;none&gt;</pre>

<p>Detailnější výpis informací o diskovém poli můžeme získat pomocí nástroje <code>mdadm</code>:</p>

<pre>debian:~# mdadm -D /dev/md2
/dev/md2:
        Version : 01.02
  Creation Time : Mon Nov  9 13:53:36 2009
     Raid Level : raid1
     Array Size : 975763866 (930.56 GiB 999.18 GB)
  Used Dev Size : 1951527732 (1861.12 GiB 1998.36 GB)
   Raid Devices : 2
  Total Devices : 2
Preferred Minor : 2
    Persistence : Superblock is persistent
    
  Intent Bitmap : Internal
  
    Update Time : Sat Jan  9 16:07:04 2010
          State : active
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0
  
           Name : 'debian':2
           UUID : 2e2fd27c:aa6c8143:b3829f5b:2e325dd8
         Events : 12
         
    Number   Major   Minor   RaidDevice State
       0       8       18        0      active sync   /dev/sdb2
       1       8        2        1      active sync   /dev/sda2</pre>

<p>Pomocí nástroje <code>mdadm</code> můžeme zkoumat i jednotlivá zařízení v poli:</p>

<pre>debian:~# mdadm -E /dev/sda2
/dev/sda2:
          Magic : a92b4efc
        Version : 1.2
    Feature Map : 0x1
     Array UUID : 2e2fd27c:aa6c8143:b3829f5b:2e325dd8
           Name : 'debian':2
  Creation Time : Mon Nov  9 13:53:36 2009
     Raid Level : raid1
   Raid Devices : 2

 Avail Dev Size : 1951527733 (930.56 GiB 999.18 GB)
     Array Size : 1951527732 (930.56 GiB 999.18 GB)
  Used Dev Size : 1951527732 (930.56 GiB 999.18 GB)
    Data Offset : 272 sectors
   Super Offset : 8 sectors
          State : clean
    Device UUID : 998eca59:96570811:c1661b8e:93707a76

Internal Bitmap : 8 sectors from superblock
    Update Time : Sat Jan  9 16:09:05 2010
       Checksum : 3860e005 - correct
         Events : 12


    Array Slot : 1 (0, 1)
   Array State : uU</pre>

<h3>Vytvoření nového pole</h3>

<p>Veškerá správa linuxového softwarového RAIDu se provádí pomocí nástroje <code>mdadm</code>. Pokud byste chtěli vytvořit RAID 5 pole se třemi zařízeními (<code>sda1</code>, <code>sdb1</code> a <code>sdc1</code>) a jednou náhradou (spare) v podobě zařízení <code>sdd1</code>, můžete použít následující příkaz:</p>

<pre>mdadm --create /dev/md0 --level=5 --raid-devices=3 /dev/sda1 /dev/sdb1 /dev/sdc1 --spare-devices=1 /dev/sdd1</pre>

<p>Může se stát, že při sestavování pole nebudete mít hned k dispozici všechny disky, v takovém případě můžete místo chybějícího disku použít klíčové slovo <code>missing</code> (chybějící):</p>

<pre>mdadm --create /dev/md0 --level=5 --raid-devices=3 /dev/sda1 missing /dev/sdc1 --spare-devices=1 /dev/sdd1</pre>

<p>Ačkoliv je možné sestavit pole v degradovaném režimu (s chybějícími redundantními aktivními zařízeními), tento postup se nedoporučuje. Zejména pak, pokud potřebujete na takové pole přesunout data ještě před tím, než doplníte zbývající zařízení.</p>

<h3>Modifikace existujícího pole</h3>

<p>Zařízení lze z pole odebírat a nová zařízení do pole vkládat. Vložení nového zařízení do pole je velmi jednoduché:</p>

<pre>mdadm --manage /dev/md0 --add /dev/sde1</pre>

<p>Každé pole má určitý definovaný počet aktivních zařízení (raid devices). Tento počet se nastavuje při vytvoření pole. Pokud je tento počet vyšší než momentální počet aktivních zařízení v poli (tj. nějaké aktivní zařízení v poli chybí, a pole je tudíž degradované), použije se nově přidané zařízení jako aktivní zařízení v poli, tedy jako náhrada za chybějící aktivní zařízení. V takové situaci dojde po zařazení daného zařízení do pole k zahájení jeho rekonstrukce.</p>

<p>Pokud je aktuální počet aktivních zařízení roven počtu aktivních zařízení v poli (žádné aktivní zařízení nechybí, pole tedy není degradované), pak se použije jako náhrada (spare). Náhradní zařízení sice v poli figurují, ale nejsou aktivní (nejsou na nich data). Teprve pokud některé z aktivních zařízení selže, pak bude náhrada automaticky zařazena do pole jako aktivní zařízení a začne jeho rekonstrukce.</p>

<p>Průběh rekonstrukce můžete sledovat třeba příkazem:</p>

<pre>watch -n 1 'cat /proc/mdstat'</pre>

<p>Odebrání zařízení z pole je trošku složitější. Záleží na tom, jestli je příslušné zařízení aktivní nebo ne. Pokud není aktivní (bylo označeno jako vadné nebo se jedná o náhradní zařízení), pak jej lze odebrat rovnou. Pokud se jedná o aktivní zařízení, je třeba jej nejdříve vyřadit z pole, což lze provést tak, že jej označíte jako vadné:</p>

<pre>mdadm --manage /dev/md0 --set-faulty /dev/sde1</pre>

<p>A následně je možné jej odebrat:</p>

<pre>mdadm --manage /dev/md0 --remove /dev/sde1</pre>

<h3>Růst pole</h3>

<p>Růst pole (grow) je operace, při které se zvýší definovaný počet aktivních zařízení v poli. Tato operace je jednorázová a nevratná. Počet aktivních zařízení v poli již nelze snížit.</p>

<p>Růst pole má dvě fáze. Nejprve je třeba do pole přidat nové zařízení, o které má pole narůst:</p>

<pre>mdadm --manage /dev/md0 --add /dev/sde1</pre>

<p>Následně se provede růst:</p>

<pre>mdadm --grow /dev/md0 --raid-devices=4</pre>

<p>Růst lze provést pouze na čistém poli, tzn. pole nesmí být degradované.</p>

<h3>Řešení krizových situací</h3>

<p>V situaci, kdy dojde k vyřazení jednoho nebo více disků z pole, je třeba zachovat rozvahu a nejprve popřemýšlet. Pokud přijdete o redundanci (tzn. selhání dalšího zařízení z pole znamená ztrátu dat), pak je vhodnější před zahájením rekonstrukce pole provést zálohu dat. Samotná rekonstrukce zbývající disky podstatně zatíží a jediná neopravitelná chyba čtení na některém z disků může situaci velice zkomplikovat.</p>

<p>Samotná výměna vadného disku je poměrně jednoduchá, postup byl naznačen v předchozí kapitole. Zařízení je obvykle již jádrem označeno jako vadné (faulty), takže postačí jej odebrat z pole a nové, funkční zařízení do pole zařadit:</p>

<pre>mdadm --manage /dev/md0 --remove /dev/sda1
mdadm --manage /dev/md0 --add /dev/sdf1</pre>

<p>Pokud víte, že disk je vadný, nebo očekáváte jeho brzké selhání, ale jádro ještě disk považuje za funkční, musíte jej za defektní nejprve označit, než se pustíme do jeho odebrání z pole:</p>

<pre>mdadm --manage /dev/md0 --set-faulty /dev/sda1</pre>

<p>Znovu raději zopakuji, že pro případ katastrofálního selhání, kdy selže krátce po sobě více disků než je pole schopné unést, nebo i pro případ chyby administrátora, atd., je nutné nebrat disková pole jako náhradu zálohování. Důležitá data je nutné pravidelně zálohovat nejlépe na nějaké geograficky odloučené místo.</p>

<p>V případě, že se dostanete do nějaké opravdu svízelné situace, kdy třeba pole nepůjde znovu sestavit, doporučuji se kromě rozvahy podívat do archívu e-mailové konference linuxového softwarového RAIDu. Tam také naleznete dostatek motivace k tomu, abyste začali svá důležitá data zálohovat, pokud to ještě neděláte.</p>

<h3>Write-intent bitmap</h3>

<p>Někdy se může stát, že některé aktivní zařízení v diskovém poli vypadne ze synchronizace s ostatními. To nemusí nutně znamenat, že disk je vadný, může se to stát třeba při nekorektním ukončení systému (způsobeném třeba výpadkem proudu), při problémech s kabeláží nebo s řadičem. Pokud k tomu dojde, je nutné pole opět zrekonstruovat, což obvykle trvá velmi dlouho.</p>

<p>Write-intent bitmap lze chápat podobně jako žurnál u souborových systémů - jádro se může takovém případě vyhnout plné synchronizaci, a synchronizovat pouze změněné oblasti, čímž se rekonstrukce podstatně zkrátí. Předpokladem je samozřejmě to, že do pole vracíme disk, který v něm již byl.</p>

<p>Pro write-intent bitmap je možné použít soubor (dle dokumentace však pouze na souborových systémech ext2 a ext3) nebo parametr "internal", který bitmapu uloží přímo na zařízení.</p>

<p>Skutečnost, že chcete použít write-intent bitmap je možné specifikovat již při vytváření pole parametrem <code>-b</code>. Pokud chcete write-intent bitmap vytvořit u již sestaveného pole, musíte použít režim "grow":</p>

<pre>mdadm --grow /dev/md0 -b internal</pre>

<p>Tímto příkazem byla vytvořena interní write-intent bitmap. Pokud se pak podíváte na <code>/proc/mdstat</code>, měli byste u příslušného zařízení vidět položku "bitmap":</p>

<pre>debian:~# cat /proc/mdstat
Personalities : [raid1] [raid6] [raid5] [raid4] 
md1 : active (auto-read-only) raid5 hda2[0] hdc2[3] hdd2[2] hdb2[1]
      2939520 blocks level 5, 64k chunk, algorithm 2 [4/4] [UUUU]
      
md0 : active raid1 hdb1[1] hdc1[2] hda1[0]
      1172608 blocks [3/3] [UUU]
      bitmap: 144/144 pages [576KB], 4KB chunk</pre>

<p>Pokud se z nějakého důvodu rozhodnete, že použijete nějaký soubor místo bitmapy interní, pak si dejte veliký pozor, aby to byl soubor na souborovém systému, který leží mimo dané pole.</p>

<h3>Scrubbing diskového pole</h3>

<p>Jelikož různé oblasti pevných disků jsou čteny a zapisovány s různou četností, může dojít k situaci, kdy některý sektor na disku v poli přestane být čitelný, aniž byste si toho byli vědomi. Následný pokus o přečtení takového sektoru, který může proběhnout i za dlouho, vyvolá neopravitelnou chybu čtení (uncorrectable read error), která vyřadí daný disk z pole.</p>

<p>Nyní uvažte situaci, kdy vám selže nějaký disk, a na některém ze zbývajících disků zůstala tato nášlapná mina v podobě nečitelného sektoru. Po vložení nového disku do pole a zahájení jeho rekonstrukce dojde na čtení z tohoto sektoru, což vyřadí další disk z pole. Pokud máte pouze jeden redundantní disk, pak jste se právě dostali do velmi nepříjemné situace.</p>

<p>Tomuto problému předcházejí hardwarové řadiče tím, že čas od času pole zkontrolují (všechno včetně redundance přečtou). Linuxový softwarový RAID má v cronu nastavené volání jednou za měsíc. Pokud vám tato frekvence nevyhovuje, můžete upravit záznam v cronu <code>/etc/cron.d/mdadm</code>, popřípadě operaci provést ručně:</p>

<pre>echo "check" &gt; /sys/block/md0/md/sync_action</pre>

<h3>Monitorování pole</h3>

<p>Disková pole je možné monitorovat stejným nástrojem (<code>mdadm</code>) jako pro jejich správu. Tento nástroj umí pracovat jako démon a hlásit události týkající se diskových polí. Jeho konfigurační soubor je <code>/etc/mdadm/mdadm.conf</code>, samotný démon by měl už rovnou běžet, pokud jste nainstalovali balíček <code>mdadm</code>.</p>

<p>Manuálová stránka konfigurační soubor velmi dobře popisuje, já tu zmíním dvě zajímavé volby:</p>

<pre>MAILADDR mail@example.com,root@localhost
PROGRAM /root/skript_spusteny_pri_udalosti</pre>

<p>Kromě obligátní e-mailové adresy, na kterou budou chodit e-maily s událostmi (předpokladem je samozřejmě funkční poštovní server), je možné specifikovat program, který <code>mdadm</code> při nějaké události spustí. Tomuto programu pak budou předány parametry zahrnující událost, zařízení odpovídající diskovému poli a, pokud to bude relevantní, označení disku v poli, kterého se událost týká.</p>

<h3>Bootování z pole</h3>

<p>GRUB 2 by měl bootování z RAIDu řešit, s GRUBem legacy (starší verze GRUBu) lze bootovat pouze z RAIDu 1, a to ještě ne ve všech případech. Zjistil jsem, že některá speciální nastavení (jiná verze superbloku, apod.) mohou bootování z RAIDu 1 zamezit. Pokud budete sestavovat pole se specifickými parametry, vyzkoušejte si, je-li možné namountovat souborový systém z jednoho z disků (či oddílů) v daném poli. Pokud se to nepovede, pak z takového pole nepůjde v GRUBu legacy nabootovat.</p>

<p>Máte-li problém s bootováním z RAIDu, nebo pokud budete nasazovat šifrování, vytvořte na všech discích malý oddíl (kolem 1GB maximálně), na kterém bude (třeba v RAIDu 1) oddíl s adresářem <code>/boot</code>, čímž vyřešíte dostupnost jádra a iniciálního ramdisku. Zbytek místa na discích pak můžete použít k vytvoření jiného typu pole.</p>

http://raid.wiki.kernel.org/ Linux Raid Wiki
http://marc.info/?l=linux-raid&amp;r=1&amp;w=2 Archív e-mailové konference linuxového softwarového RAIDu
